{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LLbBrU3Nd7_U"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Temporal Text Analysis Model*"
      ],
      "metadata": {
        "id": "IjgFSuPHdzre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Leveraging on OpenAI**"
      ],
      "metadata": {
        "id": "YpuPVeInj0o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install cohere\n",
        "!pip install tiktoken\n",
        "!pip install striprtf # to parse .rtf files\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "vNJDAqjblFMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "7rKCtzmtkFtT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Data Step**"
      ],
      "metadata": {
        "id": "gfhm6BYWZiq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import glob\n",
        "import pandas as pd\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files stored there\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the directory containing the RTF files and store all file paths\n",
        "file_paths = glob.glob('/content/drive/My Drive/Entrepreneurial Ventures/Relion AIAA/Clients/Online Education/BlackFacts/historical_docs/*.rtf')\n",
        "\n",
        "# Initialize a list to hold the data extracted from each file\n",
        "data = []\n",
        "\n",
        "# Loop through each file path in the list of file paths\n",
        "for file_path in file_paths:\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:  # Open the file for reading\n",
        "    rtf_content = file.read()  # Read the content of the file\n",
        "    content = rtf_to_text(rtf_content)  # Convert the content from RTF to plain text\n",
        "    # Append a dictionary with the file path and content to the data list\n",
        "    data.append({'file_path': file_path,\n",
        "                 'content': content\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the list of data, which contains the file paths and contents\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define a function to extract the year from the content of a document\n",
        "def extract_year(content):\n",
        "  # Find the positions of the opening and closing parentheses that enclose the year\n",
        "  start_pos = content.find('(') + 1  # Start position of the year\n",
        "  end_pos = content.find(')')  # End position of the year\n",
        "  year = content[start_pos:end_pos]  # Extract the substring that represents the year\n",
        "  return int(year)  # Convert the year to an integer and return it\n",
        "\n",
        "# Apply the function to extract the year from each document's content and store it in a new 'year' column\n",
        "df['year'] = df['content'].apply(extract_year)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "Lzv7rixqBgSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing Step**"
      ],
      "metadata": {
        "id": "HkPy6E7PaE8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary SpaCy components for natural language processing\n",
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# Load the large English model from SpaCy, which provides a rich set of features for text processing\n",
        "nlp = spacy.load('en_core_web_lg') # 'en_core_web_lg' is one of SpaCy's English models with a large vocabulary and word vectors\n",
        "\n",
        "# Define a custom attribute for Token objects to identify archaic words\n",
        "Token.set_extension('is_archaic', default=False, force=True)  # Add a new attribute 'is_archaic' to tokens\n",
        "\n",
        "# Initialize a set containing archaic words to be identified in the text\n",
        "archaic_words = {\n",
        "    'thou', 'thee', 'thy', 'thine', 'hast', 'hath', 'doth', 'dost', 'ere', 'whence', 'wherefore', 'thereunto',\n",
        "    'abide', 'afore', 'aforesaid', 'art', 'behold', 'betwixt', 'ere', 'fain', 'forsooth', 'hark', 'hither',\n",
        "    'thither', 'whence', 'wherefore', 'wherewith', 'yon', 'yonder', 'dost', 'doth', 'hath', 'hast', 'wilt',\n",
        "    'shan\\'t', 'canst', 'couldst', 'wouldst', 'shouldst', '\\'twas', '\\'tis', 'nay', 'verily', 'unto', 'upon',\n",
        "    'whereby', 'wherein', 'whereupon', 'whosoever'\n",
        "}\n",
        "\n",
        "# Define a function to check if a word is an archaic word\n",
        "def is_archaic(word):\n",
        "  # Return True if the word is in the archaic_words set, else return False\n",
        "  return word.lower() in archaic_words\n",
        "\n",
        "# Define the main preprocessing function to apply to the text\n",
        "def preprocess(text):\n",
        "  # Process the text with the SpaCy NLP model, generating a sequence of token objects\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Initialize an empty list to hold the processed tokens\n",
        "  processed_tokens = []\n",
        "\n",
        "  # Iterate over each token in the document\n",
        "  for token in doc:\n",
        "    # Ignore punctuation tokens\n",
        "    if token.is_punct:\n",
        "      continue\n",
        "\n",
        "    # Keep named entities in their original form without further processing\n",
        "    if token.ent_type_:\n",
        "      processed_tokens.append(token.text)\n",
        "      continue\n",
        "\n",
        "    # Check if the token is an archaic word and handle accordingly\n",
        "    if is_archaic(token.text):\n",
        "      # Append the token with an '_archaic' suffix for special handling\n",
        "      processed_tokens.append(token.text + \"_archaic\")\n",
        "    else:\n",
        "      # For non-archaic, non-entity tokens, lemmatize and convert to lowercase\n",
        "      processed_tokens.append(token.lemma_.lower())\n",
        "\n",
        "  # Join the processed tokens back into a single string and return it\n",
        "  return \" \".join(processed_tokens)\n",
        "\n",
        "# Apply the preprocess function to each item in the 'content' column of the dataframe and store the result in a new column\n",
        "df['processed_content'] = df['content'].apply(preprocess)"
      ],
      "metadata": {
        "id": "CwqmLRpuEGf-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Analysis and Feature Engineering Step**"
      ],
      "metadata": {
        "id": "iCuz6PkxkaDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for text analysis and feature engineering\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Advanced sentiment analysis for further iterations\n",
        "# Define a function to perform LDA topic modeling, focused on media perceptions\n",
        "def lda_topic_modeling(data, n_topics=5, n_words=10):\n",
        "    # Customize the vectorizer to better suit media-related content\n",
        "    tfidf_vect = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english', ngram_range=(1, 3))\n",
        "    dtm = tfidf_vect.fit_transform(data)\n",
        "\n",
        "    # LDA Model\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    lda.fit(dtm)\n",
        "\n",
        "    # Displaying the top words in the topics\n",
        "    for index, topic in enumerate(lda.components_):\n",
        "        print(f'The top {n_words} words for topic #{index}')\n",
        "        print([tfidf_vect.get_feature_names_out()[i] for i in topic.argsort()[-n_words:]])\n",
        "        print('\\n')\n",
        "\n",
        "# Applying LDA topic modeling to the processed content\n",
        "lda_topic_modeling(df['processed_content'].tolist())\n",
        "\n",
        "# Function to assign the dominant topic to each document\n",
        "def assign_topic(row):\n",
        "  topic = np.argmax(lda.transform(tfidf_vect.transform([row]))[0])\n",
        "  return topic\n",
        "\n",
        "# Assign the dominant topic to each document\n",
        "df['topic'] = df['processed_content'].apply(assign_topic)\n"
      ],
      "metadata": {
        "id": "W8U6qBGdFhGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for time-series analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the 'year' column is treated as an integer for proper sorting and grouping\n",
        "df['year'] = df['year'].astype(int)\n",
        "\n",
        "# Group the data by year and calculate the average sentiment for each year\n",
        "yearly_sentiment = df.groupby('year')['sentiment'].mean().reset_index()\n",
        "\n",
        "# Plot the average sentiment over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='year', y='sentiment', data=yearly_sentiment)\n",
        "plt.title('Average Sentiment Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Sentiment')\n",
        "plt.show()\n",
        "\n",
        "# Group the data by year and topic to see the distribution of topics over time\n",
        "topic_distribution = df.groupby(['year', 'topic']).size().reset_index(name='count')\n",
        "\n",
        "# Pivot the data to have years as rows, topics as columns, and counts as values\n",
        "topic_distribution_pivot = topic_distribution.pivot(index='year', columns='topic', values='count').fillna(0)\n",
        "\n",
        "# Plot the distribution of topics over time\n",
        "topic_distribution_pivot.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "plt.title('Topic Distribution Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AiWy30yEwRYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}